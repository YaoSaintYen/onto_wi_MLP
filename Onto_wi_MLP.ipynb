{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torchvision.datasets as dset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "import sys\n",
    "from collections import deque\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "individuals_file = \"individuals\"\n",
    "vocabulary       = []\n",
    "with open(individuals_file,'r') as f:\n",
    "    for line in f:\n",
    "        vocabulary.append(line.strip().split()[1])\n",
    "vocabulary_size = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_file       = \"relation_infer\"\n",
    "relations_file   = \"relations\"\n",
    "        \n",
    "wirelation     =  1\n",
    "worelation     =  0\n",
    "subject_index  =  1\n",
    "relation_index =  2\n",
    "object_index   =  3\n",
    "\n",
    "with open(relations_file,'r') as f:\n",
    "    relation_number = 0\n",
    "    for line in f:\n",
    "        relation_number += 1\n",
    "\n",
    "whole_dict     = {new_list: [] for new_list in range(relation_number)} \n",
    "\n",
    "with open(infer_file,'r') as f:\n",
    "    for line in f:\n",
    "        relation = (line.strip())\n",
    "        # label, subject, object\n",
    "        if relation.split()[0]== \"+\":\n",
    "            whole_dict[int(relation.split()[relation_index])].append([wirelation, int(relation.split()[subject_index]), int(relation.split()[object_index])])\n",
    "        else:\n",
    "            whole_dict[int(relation.split()[relation_index])].append([worelation, int(relation.split()[subject_index]), int(relation.split()[object_index])])\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_layer(word_idx):\n",
    "    x = torch.zeros(vocabulary_size).float()\n",
    "    x[word_idx] = 1.0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Onto_MLP(nn.Module):\n",
    "    def __init__(self, embedding, voca_size, hidden_size):\n",
    "        self.voca_size   = voca_size\n",
    "        self.embedding   = embedding\n",
    "        self.hidden_size = hidden_size\n",
    "        super(Onto_MLP, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Linear(self.voca_size, self.embedding),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.liner = nn.Sequential(nn.Linear(self.embedding*2, self.hidden_size), nn.Sigmoid())\n",
    "        self.out   = nn.Sequential(nn.Linear(self.hidden_size, 1), nn.Sigmoid())\n",
    "        \n",
    "    def forward_one(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        out1 = self.forward_one(x1)\n",
    "        out2 = self.forward_one(x2)\n",
    "        dis = torch.cat((out1, out2), dim=0)\n",
    "        diss = self.liner(dis)\n",
    "        out = self.out(diss)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round((y_pred))\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dims = 10\n",
    "hidden_size    = 15\n",
    "learning_rate  = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workdir2/home/sam/.pyenv/versions/3.7ver/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "loss_fn   = torch.nn.BCEWithLogitsLoss(size_average=True)\n",
    "net       = Onto_MLP(embedding_dims, vocabulary_size, hidden_size)\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr = learning_rate )\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 91.01388549804688, Loss = 0.7608086673046153\n",
      "Accuracy = 95.83333587646484, Loss = 0.6939151055448585\n",
      "Accuracy = 95.83333587646484, Loss = 0.6931759958051973\n",
      "Accuracy = 95.83333587646484, Loss = 0.6931483149114582\n",
      "Accuracy = 95.83333587646484, Loss = 0.6931472301648722\n",
      "Accuracy = 95.83333587646484, Loss = 0.6931471824645996\n",
      "Accuracy = 95.83333587646484, Loss = 0.6931471824645996\n",
      "Accuracy = 95.83333587646484, Loss = 0.6931471824645996\n",
      "Accuracy = 95.83333587646484, Loss = 0.6931471824645996\n",
      "Finish training.\n"
     ]
    }
   ],
   "source": [
    "correct     = 0\n",
    "print_epoch = 50\n",
    "train_loss  = []\n",
    "loss_val    = 0\n",
    "epoch       = 500\n",
    "threshold   = 0.5\n",
    "\n",
    "net.train()\n",
    "\n",
    "time_start  = time.time()\n",
    "\n",
    "for iteration in range(1, epoch):\n",
    "    \n",
    "    for label, su, ob in whole_dict[5]:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        su_onehot = Variable(get_input_layer(su)).float()\n",
    "        ob_onehot = Variable(get_input_layer(ob)).float()\n",
    "        output    = net.forward(su_onehot, ob_onehot)\n",
    "        loss      = loss_fn(output, torch.tensor([float(label)]))\n",
    "        loss_val  += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        output    = (output > threshold).float()\n",
    "        correct   += (output == label).float().sum()\n",
    "        \n",
    "    if iteration % print_epoch ==0 and iteration != 0:\n",
    "        \n",
    "        ae        = (loss_val/len(whole_dict[5])/print_epoch)\n",
    "        loss_val  = 0\n",
    "        accuracy  = 100 * (correct / print_epoch) / len(whole_dict[5])\n",
    "        correct   = 0\n",
    "        \n",
    "        print(\"Accuracy = {}, Loss = {}\".format(accuracy, ae))\n",
    "        \n",
    "print (\"Finish training.\")        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
